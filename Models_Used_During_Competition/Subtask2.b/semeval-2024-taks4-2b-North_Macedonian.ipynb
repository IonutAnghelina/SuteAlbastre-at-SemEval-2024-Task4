{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:37:32.661304Z","iopub.status.busy":"2024-01-11T13:37:32.660922Z","iopub.status.idle":"2024-01-11T13:37:49.305462Z","shell.execute_reply":"2024-01-11T13:37:49.304630Z","shell.execute_reply.started":"2024-01-11T13:37:32.661271Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\alex1\\anaconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","import PIL\n","import torchvision\n","import numpy\n","import pandas\n","import torch \n","import torch.optim as optim\n","import gc\n","from torch.optim.lr_scheduler import StepLR\n","import cv2\n","import os\n","import json\n","import numpy as np\n","from transformers import BertModel, BertTokenizer\n","import torch\n","import matplotlib.pyplot as plt\n","from transformers import AutoTokenizer, AutoModel\n","from transformers import T5EncoderModel\n","from transformers import GPT2Tokenizer, GPT2Model\n","from transformers import ViTImageProcessor, ViTModel\n","from PIL import Image\n","import requests\n","from tqdm import tqdm\n","import re \n","import string "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:37:49.307566Z","iopub.status.busy":"2024-01-11T13:37:49.307002Z","iopub.status.idle":"2024-01-11T13:37:49.314565Z","shell.execute_reply":"2024-01-11T13:37:49.313496Z","shell.execute_reply.started":"2024-01-11T13:37:49.307529Z"},"trusted":true},"outputs":[],"source":["PATH_DATASETS = \"../datasets\"\n","PATH_JSON_TRAIN = os.path.join(PATH_DATASETS, \"data/subtask2b/train.json\") \n","PATH_JSON_VAL = os.path.join(PATH_DATASETS, \"data/subtask2b/val.json\") \n","PATH_JSON_DEV = os.path.join(PATH_DATASETS, \"dev_gold_labels/dev_subtask2b_en.json\") \n","PATH_JSON_TEST = os.path.join(PATH_DATASETS, \"test_data/north_macedonian/mk_subtask2b_test_unlabeled.json\") \n","\n","PATH_IMG_TRAIN = os.path.join(PATH_DATASETS, \"subtask2b_images/train\") \n","PATH_IMG_VAL = os.path.join(PATH_DATASETS, \"subtask2b_images/val\") \n","PATH_IMG_DEV = os.path.join(PATH_DATASETS, \"subtask2b_images/dev\") \n","PATH_IMG_TEST = os.path.join(PATH_DATASETS, \"test_images/subtask2b/north_macedonian\") \n","\n","PATH_SAVE_MODEL = \"subtask2b_models\"\n","PATH_SAVE_SUBMISSION = \"subtask2b_submissions\"\n","\n","os.makedirs(PATH_SAVE_MODEL, exist_ok=True)\n","os.makedirs(PATH_SAVE_SUBMISSION, exist_ok=True)\n","\n","BERT_MODEL = 'macedonizer/mk-gpt2'\n","\n","BATCH_SIZE = 8\n","\n","EPOCHS_FULL = 5\n","LR_FULL = 1e-5\n","\n","EPOCHS_FC = 5\n","LR_FC = 3e-6"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:37:49.316211Z","iopub.status.busy":"2024-01-11T13:37:49.315843Z","iopub.status.idle":"2024-01-11T13:37:49.347674Z","shell.execute_reply":"2024-01-11T13:37:49.346746Z","shell.execute_reply.started":"2024-01-11T13:37:49.316177Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'id': '35807', 'text': 'DONALD TRUMP: BARACK\\\\nOBAMA AND JOE BIDEN\\\\nWILL BE IMPLICATED IN\\\\nRUSSIA HOAX\\\\nAP Photo/Pablo Martinez Monsivais', 'image': 'prop_meme_6570.png', 'label': 'propagandistic'}\n"]}],"source":["data = json.load(open(PATH_JSON_TRAIN,\"r\",encoding='utf-8'))\n","\n","print(data[0])"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:37:49.350564Z","iopub.status.busy":"2024-01-11T13:37:49.350261Z","iopub.status.idle":"2024-01-11T13:37:49.354765Z","shell.execute_reply":"2024-01-11T13:37:49.353816Z","shell.execute_reply.started":"2024-01-11T13:37:49.350532Z"},"trusted":true},"outputs":[],"source":["def preprocess(text):\n","    return text"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:37:49.356360Z","iopub.status.busy":"2024-01-11T13:37:49.356007Z","iopub.status.idle":"2024-01-11T13:37:49.364424Z","shell.execute_reply":"2024-01-11T13:37:49.363575Z","shell.execute_reply.started":"2024-01-11T13:37:49.356328Z"},"trusted":true},"outputs":[],"source":["transform = torchvision.transforms.Compose([\n","                #torchvision.transforms.ToPILImage(),\n","                #torchvision.transforms.Resize((224,224),interpolation = PIL.Image.BICUBIC),\n","                #torchvision.transforms.ToTensor(),\n","                #torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","            ])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:37:49.365731Z","iopub.status.busy":"2024-01-11T13:37:49.365473Z","iopub.status.idle":"2024-01-11T13:37:52.573184Z","shell.execute_reply":"2024-01-11T13:37:52.572250Z","shell.execute_reply.started":"2024-01-11T13:37:49.365708Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n","processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k',do_resize = True,do_rescale = True,do_normalize = True,image_mean = [0.5,0.5,0.5],image_std = [0.5,0.5,0.5])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:37:52.575363Z","iopub.status.busy":"2024-01-11T13:37:52.574617Z","iopub.status.idle":"2024-01-11T13:37:52.593355Z","shell.execute_reply":"2024-01-11T13:37:52.592423Z","shell.execute_reply.started":"2024-01-11T13:37:52.575327Z"},"trusted":true},"outputs":[],"source":["class MyDataset(Dataset):\n","    \n","    def __init__(self, paths_json_img):\n","        self.filenames = []\n","        self.texts = []\n","        self.labels = []\n","        self.images = []\n","        self.ids = []\n","        \n","        for path_json, path_img in paths_json_img:\n","            data_train = json.load(open(path_json,\"r\",encoding='utf-8'))\n","\n","            for x in tqdm(data_train):\n","                currentPath = os.path.join(path_img,x['image'])\n","\n","                self.ids.append(x['id'])\n","\n","                if 'label' in x:\n","                    if x['label']==\"non_propagandistic\":\n","                        self.labels.append(0)\n","                    else:\n","                        self.labels.append(1)\n","                else:\n","                    self.labels.append(0)\n","\n","                text = preprocess(x['text'])\n","                if text is None:\n","                    text = \"\"\n","                self.texts.append(tokenizer(text,return_tensors='pt',padding='max_length',max_length=128,truncation=True))\n","                self.filenames.append(x['image'])\n","\n","                currentImage = cv2.imread(currentPath)\n","                currentImage = torch.tensor(transform(currentImage)).unsqueeze(0)\n","                features = processor(currentImage)\n","                self.images.append(features)\n","\n","    def __len__(self):\n","        return len(self.images)\n","    \n","    def __getitem__(self,idx):\n","        \n","        text_tensors = {}\n","        for key, value in self.texts[idx].items():\n","            text_tensors[key] = value.cuda() if isinstance(value, torch.Tensor) else value\n","        \n","        \n","        image_tensors = {}\n","        for key, value in self.images[idx].items():\n","            image_tensors[key] = value.cuda() if isinstance(value, torch.Tensor) else value\n","            \n","        return ((image_tensors,text_tensors),self.labels[idx])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:37:52.594771Z","iopub.status.busy":"2024-01-11T13:37:52.594474Z","iopub.status.idle":"2024-01-11T13:39:03.158539Z","shell.execute_reply":"2024-01-11T13:39:03.157507Z","shell.execute_reply.started":"2024-01-11T13:37:52.594747Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1200/1200 [00:24<00:00, 49.09it/s]\n","100%|██████████| 150/150 [00:03<00:00, 48.67it/s]\n","100%|██████████| 300/300 [00:06<00:00, 46.10it/s]\n","100%|██████████| 150/150 [00:02<00:00, 60.83it/s]\n","100%|██████████| 100/100 [00:01<00:00, 52.49it/s]\n"]}],"source":["train_data = MyDataset([(PATH_JSON_TRAIN, PATH_IMG_TRAIN), (PATH_JSON_VAL, PATH_IMG_VAL), (PATH_JSON_DEV, PATH_IMG_DEV)])\n","valid_data = MyDataset([(PATH_JSON_VAL, PATH_IMG_VAL)])\n","test_data = MyDataset([(PATH_JSON_TEST, PATH_IMG_TEST)])\n","\n","train_dataloader = DataLoader(dataset = train_data, batch_size = BATCH_SIZE, shuffle = True)\n","valid_dataloader = DataLoader(dataset = valid_data, batch_size = BATCH_SIZE, shuffle = False)\n","test_dataloader = DataLoader(dataset = test_data, batch_size = 1, shuffle = False)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:39:03.160001Z","iopub.status.busy":"2024-01-11T13:39:03.159715Z","iopub.status.idle":"2024-01-11T13:39:03.165400Z","shell.execute_reply":"2024-01-11T13:39:03.164424Z","shell.execute_reply.started":"2024-01-11T13:39:03.159975Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","1650\n","torch.Size([1, 128])\n"]}],"source":["print(len(train_data.images[0]['pixel_values']))\n","print(len(train_data))\n","print(train_data.texts[260]['input_ids'].shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:39:03.169085Z","iopub.status.busy":"2024-01-11T13:39:03.168776Z","iopub.status.idle":"2024-01-11T13:39:16.825229Z","shell.execute_reply":"2024-01-11T13:39:16.824205Z","shell.execute_reply.started":"2024-01-11T13:39:03.169059Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\alex1\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n"]}],"source":["model_name = BERT_MODEL \n","text_tokenizer = AutoTokenizer.from_pretrained(model_name)\n","text_model = AutoModel.from_pretrained(model_name)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:39:16.827067Z","iopub.status.busy":"2024-01-11T13:39:16.826685Z","iopub.status.idle":"2024-01-11T13:39:16.840674Z","shell.execute_reply":"2024-01-11T13:39:16.837915Z","shell.execute_reply.started":"2024-01-11T13:39:16.827032Z"},"trusted":true},"outputs":[],"source":["#torchvision.models.efficientnet_b0(pretrained=True)\n","class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        # Define text and image encoders\n","        self.text_encoder = AutoModel.from_pretrained(BERT_MODEL)\n","        \n","        self.image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","        \n","        self.fc = nn.Linear(249600, 2)  # Adjust num_classes accordingly\n","        self.fc2 = nn.Linear(128,2)\n","    def forward(self,  images,text_input):\n","        # Process text input\n","        \n","        text_outputs = []\n","\n","        for i in range(text_input['input_ids'].shape[0]):\n","            x = dict()\n","            x['input_ids'] = text_input['input_ids'][i]\n","            # x['token_type_ids'] = text_input['token_type_ids'][i]\n","            x['attention_mask'] = text_input['attention_mask'][i]\n","            text_outputs.append(self.text_encoder(**x).last_hidden_state)\n","            \n","            \n","        text_outputs = torch.stack(text_outputs)\n","        image_outputs = []\n","        \n","        for i in range(images['pixel_values'][0].shape[0]):\n","            x = dict()\n","            x['pixel_values'] = images['pixel_values'][0][i].unsqueeze(0).cuda()\n","          \n","            image_outputs.append(self.image_encoder(**x).last_hidden_state)\n","        \n","        image_outputs = torch.stack(image_outputs)\n","\n","        # Flatten and concatenate the outputs\n","        text_outputs = text_outputs.view(text_outputs.size(0), -1)\n","        \n","        image_outputs = image_outputs.view(image_outputs.size(0), -1)\n","        combined = torch.cat((text_outputs, image_outputs), dim=1)\n","        \n","        # Pass through fully connected layer\n","        output = self.fc(nn.Tanh()(combined))\n","        return output"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:39:16.843428Z","iopub.status.busy":"2024-01-11T13:39:16.842131Z","iopub.status.idle":"2024-01-11T13:39:17.982589Z","shell.execute_reply":"2024-01-11T13:39:17.981530Z","shell.execute_reply.started":"2024-01-11T13:39:16.843401Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1100\n"]}],"source":["print(np.sum(train_data.labels))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:39:17.984312Z","iopub.status.busy":"2024-01-11T13:39:17.983909Z","iopub.status.idle":"2024-01-11T13:39:18.142665Z","shell.execute_reply":"2024-01-11T13:39:18.141800Z","shell.execute_reply.started":"2024-01-11T13:39:17.984284Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1650\n","1100\n"]}],"source":["total_samples = len(train_data)\n","print(total_samples)\n","print(np.sum(train_data.labels))\n","\n","class_sample_counts = [449, 900]  # Replace with your actual class sample counts\n","class_weights = [total_samples / (len(class_sample_counts) * count) for count in class_sample_counts]\n","class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda')"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:39:18.144158Z","iopub.status.busy":"2024-01-11T13:39:18.143860Z","iopub.status.idle":"2024-01-11T13:39:29.330314Z","shell.execute_reply":"2024-01-11T13:39:29.329434Z","shell.execute_reply.started":"2024-01-11T13:39:18.144124Z"},"trusted":true},"outputs":[],"source":["model = Model()\n","model.cuda()\n","model.train()\n","\n","for param in model.text_encoder.parameters():\n","    param.requires_grad = False"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:39:29.331799Z","iopub.status.busy":"2024-01-11T13:39:29.331509Z","iopub.status.idle":"2024-01-11T13:39:29.336292Z","shell.execute_reply":"2024-01-11T13:39:29.335394Z","shell.execute_reply.started":"2024-01-11T13:39:29.331774Z"},"trusted":true},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss(weight = class_weights)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:39:29.337998Z","iopub.status.busy":"2024-01-11T13:39:29.337727Z","iopub.status.idle":"2024-01-11T13:39:29.348667Z","shell.execute_reply":"2024-01-11T13:39:29.347842Z","shell.execute_reply.started":"2024-01-11T13:39:29.337975Z"},"trusted":true},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr = LR_FULL)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:39:29.350004Z","iopub.status.busy":"2024-01-11T13:39:29.349731Z","iopub.status.idle":"2024-01-11T13:46:41.883055Z","shell.execute_reply":"2024-01-11T13:46:41.882167Z","shell.execute_reply.started":"2024-01-11T13:39:29.349975Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["207it [01:02,  3.31it/s]\n","19it [00:03,  5.01it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0 Train Loss: 0.06775409778410738 Validation Loss: 0.0039329520409757445 Validation Accuracy: 88.67%\n","Checkpoint reached! Validation loss modified from 1000000000.0 to 0.0039329520409757445\n"]},{"name":"stderr","output_type":"stream","text":["207it [01:12,  2.85it/s]\n","19it [00:03,  5.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 Train Loss: 0.03636925510723482 Validation Loss: 0.002323710782961412 Validation Accuracy: 94.67%\n","Checkpoint reached! Validation loss modified from 0.0039329520409757445 to 0.002323710782961412\n"]},{"name":"stderr","output_type":"stream","text":["207it [01:06,  3.13it/s]\n","19it [00:02,  8.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 Train Loss: 0.015080504648838982 Validation Loss: 0.0009416953169486739 Validation Accuracy: 97.33%\n","Checkpoint reached! Validation loss modified from 0.002323710782961412 to 0.0009416953169486739\n"]},{"name":"stderr","output_type":"stream","text":["207it [00:58,  3.54it/s]\n","19it [00:02,  8.27it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3 Train Loss: 0.004527455367938135 Validation Loss: 0.00046591659923168747 Validation Accuracy: 98.67%\n","Checkpoint reached! Validation loss modified from 0.0009416953169486739 to 0.00046591659923168747\n"]},{"name":"stderr","output_type":"stream","text":["207it [01:05,  3.18it/s]\n","19it [00:03,  5.44it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4 Train Loss: 0.0015762402380068757 Validation Loss: 0.000332595917226916 Validation Accuracy: 98.67%\n","Checkpoint reached! Validation loss modified from 0.00046591659923168747 to 0.000332595917226916\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["best_loss = 1e9\n","\n","for epoch in range(EPOCHS_FULL):\n","\n","    train_loss = 0.0    \n","    model.train()\n","    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(train_dataloader)):\n","        optimizer.zero_grad()\n","        \n","        labels_batch = labels_batch.type(torch.LongTensor)\n","        labels_batch = labels_batch.to('cuda')\n","        \n","        labels_predictions = model(images_batch, texts_batch)\n","        \n","        loss = criterion(labels_predictions, labels_batch)\n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        train_loss = train_loss + loss.item()\n","    \n","    # Validation loop\n","    validation_loss = 0.0\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    \n","    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(valid_dataloader)):\n","        labels_batch = labels_batch.to('cuda')\n","        labels_predictions = model(images_batch, texts_batch)\n","        \n","        loss = criterion(labels_predictions, labels_batch)\n","       \n","        \n","        validation_loss = validation_loss + loss.item()\n","        \n","        \n","        _, predicted = torch.max(labels_predictions, 1)\n","        total += labels_batch.size(0)\n","        correct += (predicted == labels_batch).sum().item()\n","        \n","\n","    train_loss /= len(train_dataloader.dataset)\n","    validation_loss /= len(train_dataloader.dataset)\n","    accuracy = correct / total\n","    print(f'Epoch: {epoch} Train Loss: {train_loss} Validation Loss: {validation_loss} Validation Accuracy: {accuracy * 100:.2f}%')\n","        \n","    # Save checkpoint if needed\n","    # checkpoint = {'checkpoint': model.state_dict()}\n","    # torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'checkpoint_{epoch}.pt'))\n","    print(f'Checkpoint reached! Validation loss modified from {best_loss} to {validation_loss}')\n","    best_loss = validation_loss\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:46:41.885313Z","iopub.status.busy":"2024-01-11T13:46:41.885017Z","iopub.status.idle":"2024-01-11T13:46:41.894659Z","shell.execute_reply":"2024-01-11T13:46:41.893637Z","shell.execute_reply.started":"2024-01-11T13:46:41.885288Z"},"trusted":true},"outputs":[],"source":["for param in model.text_encoder.parameters():\n","    param.requires_grad = False\n","    \n","for param in model.image_encoder.parameters():\n","    param.requires_grad = False \n","    "]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:46:41.896267Z","iopub.status.busy":"2024-01-11T13:46:41.895937Z","iopub.status.idle":"2024-01-11T13:46:41.910475Z","shell.execute_reply":"2024-01-11T13:46:41.909598Z","shell.execute_reply.started":"2024-01-11T13:46:41.896240Z"},"trusted":true},"outputs":[],"source":["optimizer = torch.optim.Adam(model.parameters(), lr = LR_FC)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:46:41.913549Z","iopub.status.busy":"2024-01-11T13:46:41.913203Z","iopub.status.idle":"2024-01-11T13:49:29.094014Z","shell.execute_reply":"2024-01-11T13:49:29.093043Z","shell.execute_reply.started":"2024-01-11T13:46:41.913516Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["207it [00:24,  8.53it/s]\n","19it [00:02,  8.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 0 Train Loss: 0.0007350683830714622 Validation Loss: 0.00019417805473715292 Validation Accuracy: 98.67%\n","Checkpoint reached! Validation loss modified from 1000000000.0 to 0.00019417805473715292\n"]},{"name":"stderr","output_type":"stream","text":["207it [00:23,  8.68it/s]\n","19it [00:02,  8.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 1 Train Loss: 0.0004821440336915354 Validation Loss: 0.00022143103588005583 Validation Accuracy: 98.67%\n","Checkpoint reached! Validation loss modified from 0.00019417805473715292 to 0.00022143103588005583\n"]},{"name":"stderr","output_type":"stream","text":["207it [00:23,  8.64it/s]\n","19it [00:02,  8.92it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 2 Train Loss: 0.0006059995553126403 Validation Loss: 0.0002158653073252715 Validation Accuracy: 99.33%\n","Checkpoint reached! Validation loss modified from 0.00022143103588005583 to 0.0002158653073252715\n"]},{"name":"stderr","output_type":"stream","text":["207it [00:23,  8.70it/s]\n","19it [00:02,  8.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch: 3 Train Loss: 0.0005452822690131143 Validation Loss: 0.0002464278722464135 Validation Accuracy: 99.33%\n","Checkpoint reached! Validation loss modified from 0.0002158653073252715 to 0.0002464278722464135\n"]},{"name":"stderr","output_type":"stream","text":["207it [00:26,  7.90it/s]\n","19it [00:03,  6.19it/s]"]},{"name":"stdout","output_type":"stream","text":["Epoch: 4 Train Loss: 0.00041467505705826847 Validation Loss: 0.0001762576583220807 Validation Accuracy: 99.33%\n","Checkpoint reached! Validation loss modified from 0.0002464278722464135 to 0.0001762576583220807\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["best_loss = 1e9\n","\n","\n","for epoch in range(EPOCHS_FC):\n","\n","    train_loss = 0.0    \n","    model.train()\n","    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(train_dataloader)):\n","        optimizer.zero_grad()\n","        \n","        labels_batch = labels_batch.type(torch.LongTensor)\n","        labels_batch = labels_batch.to('cuda')\n","        \n","        labels_predictions = model(images_batch, texts_batch)\n","        \n","        loss = criterion(labels_predictions, labels_batch)\n","        loss.backward()\n","        \n","        optimizer.step()\n","        \n","        train_loss = train_loss + loss.item()\n","    \n","    # Validation loop\n","    validation_loss = 0.0\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    \n","    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(valid_dataloader)):\n","        labels_batch = labels_batch.to('cuda')\n","        \n","        labels_predictions = model(images_batch, texts_batch)\n","        \n","        loss = criterion(labels_predictions, labels_batch)\n","       \n","        \n","        validation_loss = validation_loss + loss.item()\n","        \n","        \n","        _, predicted = torch.max(labels_predictions, 1)\n","        total += labels_batch.size(0)\n","        correct += (predicted == labels_batch).sum().item()\n","        \n","\n","    train_loss /= len(train_dataloader.dataset)\n","    validation_loss /= len(train_dataloader.dataset)\n","    accuracy = correct / total\n","    print(f'Epoch: {epoch} Train Loss: {train_loss} Validation Loss: {validation_loss} Validation Accuracy: {accuracy * 100:.2f}%')\n","        \n","    # Save checkpoint if needed\n","    # checkpoint = {'checkpoint': model.state_dict()}\n","    # torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'fc_checkpoint_{epoch}.pt'))\n","    print(f'Checkpoint reached! Validation loss modified from {best_loss} to {validation_loss}')\n","    best_loss = validation_loss\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:49:29.095965Z","iopub.status.busy":"2024-01-11T13:49:29.095496Z","iopub.status.idle":"2024-01-11T13:49:30.744486Z","shell.execute_reply":"2024-01-11T13:49:30.743457Z","shell.execute_reply.started":"2024-01-11T13:49:29.095928Z"},"trusted":true},"outputs":[],"source":["checkpoint = {'checkpoint': model.state_dict()}\n","torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'checkpoint.pt'))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:49:30.746276Z","iopub.status.busy":"2024-01-11T13:49:30.745945Z","iopub.status.idle":"2024-01-11T13:49:30.750504Z","shell.execute_reply":"2024-01-11T13:49:30.749557Z","shell.execute_reply.started":"2024-01-11T13:49:30.746250Z"},"trusted":true},"outputs":[],"source":["#import torch\n","# model.train()\n","# checkpoint = torch.load(os.path.join(PATH_SAVE_MODEL, f'fc_checkpoint_{4}.pt'))\n","\n","# # Apply the state dictionary to the model\n","# model.load_state_dict(checkpoint['checkpoint'])"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:49:30.752563Z","iopub.status.busy":"2024-01-11T13:49:30.751999Z","iopub.status.idle":"2024-01-11T13:49:38.394360Z","shell.execute_reply":"2024-01-11T13:49:38.393413Z","shell.execute_reply.started":"2024-01-11T13:49:30.752535Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100it [00:02, 44.31it/s]\n"]}],"source":["predictions = []\n","ids = []\n","\n","for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(test_dataloader)):\n","    model.eval()\n","    labels_batch = labels_batch.type(torch.LongTensor)\n","        \n","    # Move data to GPU\n","    #images_batch = images_batch.to('cuda')\n","    #texts_batch = texts_batch.to(device)\n","    labels_batch = labels_batch.to('cuda')\n","\n","    labels_predictions = model(images_batch, texts_batch)\n","\n","\n","    _, predicted = torch.max(labels_predictions, 1)\n","\n","    predictions.append(predicted.item())\n","    ids.append(test_data.ids[useless_id])"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T13:49:38.396061Z","iopub.status.busy":"2024-01-11T13:49:38.395732Z","iopub.status.idle":"2024-01-11T13:49:38.406695Z","shell.execute_reply":"2024-01-11T13:49:38.405893Z","shell.execute_reply.started":"2024-01-11T13:49:38.396035Z"},"trusted":true},"outputs":[],"source":["with open(os.path.join(PATH_SAVE_SUBMISSION, \"submission.txt\"),\"w\") as fout:\n","    print(\"[\\n\",end='',file=fout)\n","    \n","    idx = 0\n","    for (ID,pred) in zip(ids,predictions):\n","        idx += 1\n","        predName = None\n","        if pred==0:\n","            predName = 'non_propagandistic'\n","        else:\n","            predName = 'propagandistic'\n","            \n","        ID = \"\\\"\" + ID + \"\\\"\"\n","        predName = \"\\\"\" + predName + \"\\\"\"\n","\n","        print(\"{\\n\\\"id\\\":\",end='',file=fout)\n","        print(f\" {ID},\\n\",end='',file=fout)\n","        print(\"\\\"label\\\":\",end='',file=fout)\n","        print(f\" {predName}\\n\",end='',file=fout)\n","        \n","        \n","        if idx < len(predictions):\n","            print(\"},\",file=fout)\n","        else:\n","            print(\"}\",file=fout)\n","    print(']',file=fout)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4291150,"sourceId":7383307,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}
