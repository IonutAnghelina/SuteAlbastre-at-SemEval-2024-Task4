{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7519934,"sourceType":"datasetVersion","datasetId":4291150}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport PIL\nimport torchvision\nimport numpy\nimport pandas\nimport torch \nimport torch.optim as optim\nimport gc\nfrom torch.optim.lr_scheduler import StepLR\nimport cv2\nimport os\nimport json\nimport numpy as np\nfrom transformers import BertModel, BertTokenizer\nimport torch\nimport matplotlib.pyplot as plt\nfrom transformers import AutoTokenizer, AutoModel\nfrom transformers import T5EncoderModel\nfrom transformers import GPT2Tokenizer, GPT2Model\nfrom transformers import ViTImageProcessor, ViTModel\nfrom PIL import Image\nimport requests\nfrom tqdm import tqdm\nimport re \nimport string ","metadata":{"execution":{"iopub.status.busy":"2024-02-16T18:53:18.724048Z","iopub.execute_input":"2024-02-16T18:53:18.724922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH_DATASETS = \".\"\nPATH_JSON_TRAIN = os.path.join(PATH_DATASETS, \"annotations/data/subtask1/train.json\") \nPATH_JSON_VAL = os.path.join(PATH_DATASETS, \"annotations/data/subtask1/validation.json\") \nPATH_JSON_DEV = os.path.join(PATH_DATASETS, \"dev_gold_labels/dev_gold_labels/dev_subtask1_en.json\") \n\nPATH_SAVE_MODEL = \"subtask1_models\"\nPATH_SAVE_SUBMISSION = \"subtask1_submissions\"\n\nos.makedirs(PATH_SAVE_MODEL, exist_ok=True)\nos.makedirs(PATH_SAVE_SUBMISSION, exist_ok=True)\n\nBATCH_SIZE = 8\n\nEPOCHS_FULL = 1\nLR_FULL = 1e-5\n\nEPOCHS_FC = 1\nLR_FC = 3e-6\n\nTRAIN_ALL = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = json.load(open(PATH_JSON_TRAIN,\"r\",encoding='utf-8'))\nmodel_name = 'asafaya/bert-base-arabic'  \ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntext_model = AutoModel.from_pretrained(model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    \n    def __init__(self, paths_json, bin_classes):\n        self.texts = []\n        self.ids = []\n        self.labels = []\n        \n        for path_json in paths_json:\n            data = json.load(open(path_json,\"r\",encoding='utf-8'))\n\n            for x in tqdm(data):\n                self.ids.append(x['id'])\n\n                if 'labels' in x:\n                    curr_labels = []\n                    for bin_class in bin_classes:\n                        if bin_class in x['labels']:\n                            curr_labels.append(1)\n                        else:\n                            curr_labels.append(0)\n                    self.labels.append(curr_labels)\n                else:\n                    self.labels.append([])\n\n                text = x['text']\n                if text is None:\n                    text = \"\"\n                self.texts.append(tokenizer(text,return_tensors='pt',padding='max_length',max_length=128,truncation=True))\n\n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self,idx):\n        text_tensors = {}\n        for key, value in self.texts[idx].items():\n            text_tensors[key] = value.cuda() if isinstance(value, torch.Tensor) else value\n            \n            ##########################\n            # text_tensors[key] = value\n        \n        return (text_tensors,torch.tensor(self.labels[idx]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torchvision.models.efficientnet_b0(pretrained=True)\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        # Define text and image encoders\n        self.text_encoder = AutoModel.from_pretrained('asafaya/bert-base-arabic')\n        \n        self.fc = nn.Linear(98304, 20)  # Adjust num_classes accordingly\n    def forward(self,text_input):\n        # Process text input\n        \n        text_outputs = []\n\n        for i in range(text_input['input_ids'].shape[0]):\n            x = dict()\n            x['input_ids'] = text_input['input_ids'][i]\n            x['token_type_ids'] = text_input['token_type_ids'][i]\n            x['attention_mask'] = text_input['attention_mask'][i]\n            text_outputs.append(self.text_encoder(**x).last_hidden_state)\n            \n            \n        text_outputs = torch.stack(text_outputs)\n        # Flatten and concatenate the outputs\n        text_outputs = text_outputs.view(text_outputs.size(0), -1)\n        \n        # Pass through fully connected layer\n        output = nn.Sigmoid()(self.fc(nn.Tanh()(text_outputs)))\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = json.load(open(PATH_JSON_TRAIN,\"r\",encoding='utf-8'))\n\nbin_classes = []\n\nfor x in data:\n    for label in x['labels']:\n        if label not in bin_classes:\n            bin_classes.append(label)\n\nprint(len(bin_classes))\nprint(bin_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN_ALL:\n    train_data = MyDataset([PATH_JSON_TRAIN, PATH_JSON_VAL], bin_classes)\nelse:\n    train_data = MyDataset([PATH_JSON_TRAIN], bin_classes)\nvalid_data = MyDataset([PATH_JSON_VAL], bin_classes)\ntest_data = MyDataset([PATH_JSON_DEV], bin_classes)\n\ntrain_dataloader = DataLoader(dataset = train_data, batch_size = BATCH_SIZE, shuffle = True)\nvalid_dataloader = DataLoader(dataset = valid_data, batch_size = BATCH_SIZE, shuffle = False)\ntest_dataloader = DataLoader(dataset = test_data, batch_size = 1, shuffle = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = {}\n    \nprint(len(train_data))\nprint(train_data.texts[0]['input_ids'].shape)\n\nmodel = Model()\nmodel.cuda()\nmodel.train()\n\ncriterion = torch.nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = LR_FULL)\n\nbest_loss = 1e9\n\nfor epoch in range(EPOCHS_FULL):\n\n    train_loss = 0.0    \n    model.train()\n    for useless_id, (texts_batch, labels_batch) in tqdm(enumerate(train_dataloader)):\n        optimizer.zero_grad()\n\n        labels_batch = labels_batch.to(torch.float32)\n        labels_batch = labels_batch.to('cuda')\n\n        labels_predictions = model(texts_batch)\n\n        loss = criterion(labels_predictions, labels_batch)\n        loss.backward()\n\n        optimizer.step()\n\n        train_loss = train_loss + loss.item()\n\n    # Validation loop\n    validation_loss = 0.0\n    model.eval()\n    correct = 0\n    total = 0\n\n    for useless_id, (texts_batch, labels_batch) in tqdm(enumerate(valid_dataloader)):\n        labels_batch = labels_batch.to(torch.float32)\n        labels_batch = labels_batch.to('cuda')\n        labels_predictions = model(texts_batch)\n\n        loss = criterion(labels_predictions, labels_batch)\n\n\n        validation_loss = validation_loss + loss.item()\n\n\n        predicted = (labels_predictions > 0.5)\n        \n        total += labels_batch.size(0)\n        correct += (predicted == labels_batch).sum().item()\n\n\n    train_loss /= len(train_dataloader.dataset)\n    validation_loss /= len(train_dataloader.dataset)\n    accuracy = (correct / total) / len(bin_classes)\n    print(f'Epoch: {epoch} Train Loss: {train_loss} Validation Loss: {validation_loss} Validation Accuracy: {accuracy * 100:.2f}%')\n\n    # Save checkpoint if needed\n    # checkpoint = {'checkpoint': model.state_dict()}\n    # torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'checkpoint_{epoch}.pt'))\n    print(f'Checkpoint reached! Validation loss modified from {best_loss} to {validation_loss}')\n    best_loss = validation_loss\n    torch.cuda.empty_cache()\n                    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in model.text_encoder.parameters():\n    param.requires_grad = False\n\noptimizer = torch.optim.Adam(model.parameters(), lr = LR_FC)\nbest_loss = 1e9\n\n\nfor epoch in range(EPOCHS_FC):\n\n    train_loss = 0.0    \n    model.train()\n    for useless_id, (texts_batch, labels_batch) in tqdm(enumerate(train_dataloader)):\n        optimizer.zero_grad()\n\n        labels_batch = labels_batch.to(torch.float32)\n        labels_batch = labels_batch.to('cuda')\n\n        labels_predictions = model(texts_batch)\n\n        loss = criterion(labels_predictions, labels_batch)\n        loss.backward()\n\n        optimizer.step()\n\n        train_loss = train_loss + loss.item()\n\n    # Validation loop\n    validation_loss = 0.0\n    model.eval()\n    correct = 0\n    total = 0\n\n    for useless_id, (texts_batch, labels_batch) in tqdm(enumerate(valid_dataloader)):\n        labels_batch = labels_batch.to(torch.float32)\n        labels_batch = labels_batch.to('cuda')\n\n        labels_predictions = model(texts_batch)\n\n        loss = criterion(labels_predictions, labels_batch)\n\n\n        validation_loss = validation_loss + loss.item()\n\n\n        predicted = (labels_predictions > 0.5)\n        total += labels_batch.size(0)\n        correct += (predicted == labels_batch).sum().item()\n\n\n    train_loss /= len(train_dataloader.dataset)\n    validation_loss /= len(train_dataloader.dataset)\n    accuracy = (correct / total) / len(bin_classes)\n    print(f'Epoch: {epoch} Train Loss: {train_loss} Validation Loss: {validation_loss} Validation Accuracy: {accuracy * 100:.2f}%')\n\n\n\n\n\n    # Save checkpoint if needed\n    # checkpoint = {'checkpoint': model.state_dict()}\n    # torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'fc_checkpoint_{epoch}.pt'))\n    print(f'Checkpoint reached! Validation loss modified from {best_loss} to {validation_loss}')\n    best_loss = validation_loss\n    torch.cuda.empty_cache()\n\n    checkpoint = {'checkpoint': model.state_dict()}\n    torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'checkpoint.pt'))\n\n    #import torch\n    # model.train()\n    # checkpoint = torch.load(os.path.join(PATH_SAVE_MODEL, f'fc_checkpoint_{4}.pt'))\n\n    # # Apply the state dictionary to the model\n    # model.load_state_dict(checkpoint['checkpoint'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = {}\nids = []\n\nfor useless_id, (texts_batch, labels_batch) in tqdm(enumerate(test_dataloader)):\n    model.eval()\n\n    labels_predictions = model(texts_batch)\n\n\n    predicted = (labels_predictions > 0.25)[0]\n    \n    curr_id = test_data.ids[useless_id]\n    if curr_id not in predictions:\n        predictions[curr_id] = []\n        \n    idx_bin_class = 0\n    for bin_class in bin_classes:\n        if predicted[idx_bin_class]:\n            predictions[curr_id].append(bin_class)\n        idx_bin_class += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_json = []\nfor k,v in predictions.items():\n    output_json.append({\"id\" : k, \"labels\" : v})\n\nwith open(\"submission_asafaya_bert-base-arabic_1.txt\",\"w\") as fout:\n    json.dump(output_json, fout)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}
