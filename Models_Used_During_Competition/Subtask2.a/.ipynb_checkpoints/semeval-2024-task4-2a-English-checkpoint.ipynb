{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T21:52:09.798290Z",
     "iopub.status.busy": "2024-01-11T21:52:09.797912Z",
     "iopub.status.idle": "2024-01-11T21:52:25.519689Z",
     "shell.execute_reply": "2024-01-11T21:52:25.518909Z",
     "shell.execute_reply.started": "2024-01-11T21:52:09.798259Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ionut\\anaconda3\\envs\\mlenvfinal\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import PIL\n",
    "import torchvision\n",
    "import numpy\n",
    "import pandas\n",
    "import torch \n",
    "import torch.optim as optim\n",
    "import gc\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import T5EncoderModel\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from PIL import Image\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import re \n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T21:52:25.521697Z",
     "iopub.status.busy": "2024-01-11T21:52:25.521105Z",
     "iopub.status.idle": "2024-01-11T21:52:25.529153Z",
     "shell.execute_reply": "2024-01-11T21:52:25.528233Z",
     "shell.execute_reply.started": "2024-01-11T21:52:25.521671Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH_DATASETS = \".\"\n",
    "PATH_JSON_TRAIN = os.path.join(PATH_DATASETS, \"annotations/data/subtask2a/train.json\") \n",
    "PATH_JSON_VAL = os.path.join(PATH_DATASETS, \"annotations/data/subtask2a/validation.json\") \n",
    "\n",
    "PATH_JSON_DEV = os.path.join(PATH_DATASETS, \"annotations/data/subtask2a/dev_subtask2a_en.json\") \n",
    "PATH_JSON_TEST = \"./test_data/english/en_subtask2a_test_unlabeled.json\"\n",
    "\n",
    "#os.path.join(PATH_DATASETS, \"./test_data_arabic/test_data_arabic/ar_subtask2a_test_unlabeled.json\") \n",
    "\n",
    "\n",
    "PATH_IMG_TRAIN = \"./train_images\"\n",
    "PATH_IMG_VAL = \"./validation_images\"\n",
    "PATH_IMG_DEV = \"./dev_images\"\n",
    "PATH_IMG_TEST = \"./test_images/subtask1_2a/english\"\n",
    "\n",
    "PATH_SAVE_MODEL = \"subtask2a_models\"\n",
    "PATH_SAVE_SUBMISSION = \"subtask2a_submissions\"\n",
    "\n",
    "os.makedirs(PATH_SAVE_MODEL, exist_ok=True)\n",
    "os.makedirs(PATH_SAVE_SUBMISSION, exist_ok=True)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "EPOCHS_FULL = 3\n",
    "LR_FULL = 1e-5\n",
    "\n",
    "EPOCHS_FC = 3\n",
    "LR_FC = 3e-6\n",
    "\n",
    "TRAIN_ALL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T21:52:25.530769Z",
     "iopub.status.busy": "2024-01-11T21:52:25.530501Z",
     "iopub.status.idle": "2024-01-11T21:52:25.681833Z",
     "shell.execute_reply": "2024-01-11T21:52:25.680947Z",
     "shell.execute_reply.started": "2024-01-11T21:52:25.530747Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '63292', 'text': \"This is why we're free\\\\n\\\\nThis is why we're safe\\\\n\", 'image': 'prop_meme_556.png', 'labels': ['Causal Oversimplification', 'Transfer', 'Flag-waving'], 'link': 'https://www.facebook.com/SilentmajorityDJT/photos/2119966118152814/'}\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(PATH_JSON_TRAIN,\"r\",encoding='utf-8'))\n",
    "\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T21:52:25.685181Z",
     "iopub.status.busy": "2024-01-11T21:52:25.684436Z",
     "iopub.status.idle": "2024-01-11T21:52:25.689155Z",
     "shell.execute_reply": "2024-01-11T21:52:25.688312Z",
     "shell.execute_reply.started": "2024-01-11T21:52:25.685143Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T21:52:25.690530Z",
     "iopub.status.busy": "2024-01-11T21:52:25.690239Z",
     "iopub.status.idle": "2024-01-11T21:52:25.709678Z",
     "shell.execute_reply": "2024-01-11T21:52:25.708819Z",
     "shell.execute_reply.started": "2024-01-11T21:52:25.690506Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "                #torchvision.transforms.ToPILImage(),\n",
    "                #torchvision.transforms.Resize((224,224),interpolation = PIL.Image.BICUBIC),\n",
    "                #torchvision.transforms.ToTensor(),\n",
    "                #torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-11T21:52:25.711065Z",
     "iopub.status.busy": "2024-01-11T21:52:25.710794Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ionut\\anaconda3\\envs\\mlenvfinal\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertModel were not initialized from the model checkpoint at limjiayi/bert-hateful-memes-expanded and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k',do_resize = True,do_rescale = True,do_normalize = True,image_mean = [0.5,0.5,0.5],image_std = [0.5,0.5,0.5])\n",
    "\n",
    "model_name = 'limjiayi/bert-hateful-memes-expanded'  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text_model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, paths_json_img, bin_classes):\n",
    "        self.filenames = []\n",
    "        self.texts = []\n",
    "        self.images = []\n",
    "        self.ids = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for path_json, path_img in paths_json_img:\n",
    "            data = json.load(open(path_json,\"r\",encoding='utf-8'))\n",
    "\n",
    "            for x in tqdm(data):\n",
    "                currentPath = os.path.join(path_img,x['image'])\n",
    "\n",
    "                self.ids.append(x['id'])\n",
    "\n",
    "                if 'labels' in x:\n",
    "                    curr_labels = []\n",
    "                    for bin_class in bin_classes:\n",
    "                        if bin_class in x['labels']:\n",
    "                            curr_labels.append(1)\n",
    "                        else:\n",
    "                            curr_labels.append(0)\n",
    "                    self.labels.append(curr_labels)\n",
    "                else:\n",
    "                    self.labels.append([])\n",
    "\n",
    "                text = preprocess(x['text'])\n",
    "                if text is None:\n",
    "                    text = \"\"\n",
    "                self.texts.append(tokenizer(text,return_tensors='pt',padding='max_length',max_length=128,truncation=True))\n",
    "                self.filenames.append(x['image'])\n",
    "\n",
    "                currentImage = cv2.imread(currentPath)\n",
    "                currentImage = torch.tensor(transform(currentImage)).unsqueeze(0)\n",
    "                features = processor(currentImage)\n",
    "                self.images.append(features)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        text_tensors = {}\n",
    "        for key, value in self.texts[idx].items():\n",
    "            text_tensors[key] = value.cuda() if isinstance(value, torch.Tensor) else value\n",
    "        \n",
    "        \n",
    "        image_tensors = {}\n",
    "        for key, value in self.images[idx].items():\n",
    "            image_tensors[key] = value.cuda() if isinstance(value, torch.Tensor) else value\n",
    "        \n",
    "        return ((image_tensors,text_tensors),torch.tensor(self.labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchvision.models.efficientnet_b0(pretrained=True)\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # Define text and image encoders\n",
    "        self.text_encoder = AutoModel.from_pretrained('limjiayi/bert-hateful-memes-expanded')\n",
    "        \n",
    "        self.image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "        \n",
    "        self.fc = nn.Linear(249600, 22)  # Adjust num_classes accordingly\n",
    "        self.fc2 = nn.Linear(128,2)\n",
    "    def forward(self,  images,text_input):\n",
    "        # Process text input\n",
    "        \n",
    "        text_outputs = []\n",
    "\n",
    "        for i in range(text_input['input_ids'].shape[0]):\n",
    "            x = dict()\n",
    "            x['input_ids'] = text_input['input_ids'][i]\n",
    "            x['token_type_ids'] = text_input['token_type_ids'][i]\n",
    "            x['attention_mask'] = text_input['attention_mask'][i]\n",
    "            text_outputs.append(self.text_encoder(**x).last_hidden_state)\n",
    "            \n",
    "            \n",
    "        text_outputs = torch.stack(text_outputs)\n",
    "        image_outputs = []\n",
    "        \n",
    "        for i in range(images['pixel_values'][0].shape[0]):\n",
    "            x = dict()\n",
    "            x['pixel_values'] = images['pixel_values'][0][i].unsqueeze(0).cuda()\n",
    "          \n",
    "            image_outputs.append(self.image_encoder(**x).last_hidden_state)\n",
    "        \n",
    "        image_outputs = torch.stack(image_outputs)\n",
    "\n",
    "     \n",
    "        text_outputs = text_outputs.view(text_outputs.size(0), -1)\n",
    "        \n",
    "        image_outputs = image_outputs.view(image_outputs.size(0), -1)\n",
    "        combined = torch.cat((text_outputs, image_outputs), dim=1)\n",
    "        \n",
    "     \n",
    "        output = nn.Sigmoid()(self.fc(nn.Tanh()(combined)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "['Causal Oversimplification', 'Transfer', 'Flag-waving', 'Black-and-white Fallacy/Dictatorship', 'Smears', 'Loaded Language', 'Glittering generalities (Virtue)', 'Thought-terminating cliché', 'Whataboutism', 'Slogans', 'Doubt', 'Name calling/Labeling', 'Repetition', 'Appeal to authority', 'Appeal to (Strong) Emotions', 'Reductio ad hitlerum', 'Appeal to fear/prejudice', 'Exaggeration/Minimisation', \"Misrepresentation of Someone's Position (Straw Man)\", 'Obfuscation, Intentional vagueness, Confusion', 'Bandwagon', 'Presenting Irrelevant Data (Red Herring)']\n"
     ]
    }
   ],
   "source": [
    "data = json.load(open(PATH_JSON_TRAIN,\"r\",encoding='utf-8'))\n",
    "\n",
    "bin_classes = []\n",
    "\n",
    "for x in data:\n",
    "    for label in x['labels']:\n",
    "        if label not in bin_classes:\n",
    "            bin_classes.append(label)\n",
    "\n",
    "print(len(bin_classes))\n",
    "print(bin_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████                                         | 3325/7000 [01:11<01:18, 46.60it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\importlib\\metadata\\__init__.py:807\u001b[0m, in \u001b[0;36mFastPath.mtime\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mOSError\u001b[39;00m):\n\u001b[1;32m--> 807\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_mtime\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup\u001b[38;5;241m.\u001b[39mcache_clear()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\ionut\\\\anaconda3\\\\envs\\\\mlenvfinal\\\\python310.zip'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TRAIN_ALL:\n\u001b[1;32m----> 2\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m \u001b[43mMyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_JSON_TRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH_IMG_TRAIN\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_JSON_VAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH_IMG_VAL\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_JSON_DEV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPATH_IMG_DEV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbin_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m MyDataset([(PATH_JSON_TRAIN, PATH_IMG_TRAIN)], bin_classes)\n",
      "Cell \u001b[1;32mIn[8], line 37\u001b[0m, in \u001b[0;36mMyDataset.__init__\u001b[1;34m(self, paths_json_img, bin_classes)\u001b[0m\n\u001b[0;32m     35\u001b[0m currentImage \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(currentPath)\n\u001b[0;32m     36\u001b[0m currentImage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(transform(currentImage))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrentImage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages\u001b[38;5;241m.\u001b[39mappend(features)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\site-packages\\transformers\\image_processing_utils.py:552\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m    551\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 552\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\site-packages\\transformers\\models\\vit\\image_processing_vit.py:219\u001b[0m, in \u001b[0;36mViTImageProcessor.preprocess\u001b[1;34m(self, images, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, return_tensors, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[0;32m    215\u001b[0m size_dict \u001b[38;5;241m=\u001b[39m get_size_dict(size)\n\u001b[0;32m    217\u001b[0m images \u001b[38;5;241m=\u001b[39m make_list_of_images(images)\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvalid_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    223\u001b[0m     )\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_resize \u001b[38;5;129;01mand\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\site-packages\\transformers\\image_utils.py:104\u001b[0m, in \u001b[0;36mvalid_images\u001b[1;34m(imgs)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(imgs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m imgs:\n\u001b[1;32m--> 104\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mvalid_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# If not a list of tuple, we have been given a single image or batched tensor of images\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\site-packages\\transformers\\image_utils.py:107\u001b[0m, in \u001b[0;36mvalid_images\u001b[1;34m(imgs)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# If not a list of tuple, we have been given a single image or batched tensor of images\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_valid_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\site-packages\\transformers\\image_utils.py:92\u001b[0m, in \u001b[0;36mis_valid_image\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_image\u001b[39m(img):\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m---> 92\u001b[0m         (\u001b[43mis_vision_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage))\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, np\u001b[38;5;241m.\u001b[39mndarray)\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_torch_tensor(img)\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_tf_tensor(img)\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m is_jax_tensor(img)\n\u001b[0;32m     97\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\site-packages\\transformers\\utils\\import_utils.py:729\u001b[0m, in \u001b[0;36mis_vision_available\u001b[1;34m()\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _pil_available:\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         package_version \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPillow\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mPackageNotFoundError:\n\u001b[0;32m    731\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\importlib\\metadata\\__init__.py:996\u001b[0m, in \u001b[0;36mversion\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mversion\u001b[39m(distribution_name):\n\u001b[0;32m    990\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \n\u001b[0;32m    992\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;124;03m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;124;03m        \"Version\" metadata key.\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mversion\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\importlib\\metadata\\__init__.py:969\u001b[0m, in \u001b[0;36mdistribution\u001b[1;34m(distribution_name)\u001b[0m\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdistribution\u001b[39m(distribution_name):\n\u001b[0;32m    964\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[0;32m    965\u001b[0m \n\u001b[0;32m    966\u001b[0m \u001b[38;5;124;03m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistribution_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\importlib\\metadata\\__init__.py:544\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[1;34m(cls, name)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m resolver \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_discover_resolvers():\n\u001b[0;32m    543\u001b[0m     dists \u001b[38;5;241m=\u001b[39m resolver(DistributionFinder\u001b[38;5;241m.\u001b[39mContext(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m--> 544\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdists\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dist \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m dist\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\importlib\\metadata\\__init__.py:904\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Find metadata directories in paths heuristically.\"\"\"\u001b[39;00m\n\u001b[0;32m    902\u001b[0m prepared \u001b[38;5;241m=\u001b[39m Prepared(name)\n\u001b[0;32m    903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m--> 904\u001b[0m     \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepared\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(FastPath, paths)\n\u001b[0;32m    905\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\importlib\\metadata\\__init__.py:802\u001b[0m, in \u001b[0;36mFastPath.search\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m--> 802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmtime\u001b[49m)\u001b[38;5;241m.\u001b[39msearch(name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlenvfinal\\lib\\importlib\\metadata\\__init__.py:807\u001b[0m, in \u001b[0;36mFastPath.mtime\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmtime\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mOSError\u001b[39;00m):\n\u001b[1;32m--> 807\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mst_mtime\n\u001b[0;32m    808\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlookup\u001b[38;5;241m.\u001b[39mcache_clear()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN_ALL:\n",
    "    train_data = MyDataset([(PATH_JSON_TRAIN, PATH_IMG_TRAIN), (PATH_JSON_VAL, PATH_IMG_VAL),(PATH_JSON_DEV, PATH_IMG_DEV)], bin_classes)\n",
    "else:\n",
    "    train_data = MyDataset([(PATH_JSON_TRAIN, PATH_IMG_TRAIN)], bin_classes)\n",
    "valid_data = MyDataset([(PATH_JSON_VAL, PATH_IMG_VAL)], bin_classes)\n",
    "test_data = MyDataset([(PATH_JSON_TEST, PATH_IMG_TEST)], bin_classes)\n",
    "\n",
    "train_dataloader = DataLoader(dataset = train_data, batch_size = BATCH_SIZE, shuffle = True)\n",
    "valid_dataloader = DataLoader(dataset = valid_data, batch_size = BATCH_SIZE, shuffle = False)\n",
    "test_dataloader = DataLoader(dataset = test_data, batch_size = 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = nn.Sigmoid()\n",
    "# loss = nn.BCELoss()\n",
    "# input = torch.randn(3, requires_grad=True)\n",
    "# target = torch.empty(3).random_(2)\n",
    "\n",
    "# print(input)\n",
    "# print(target)\n",
    "# output = loss(m(input), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "    \n",
    "print(len(train_data.images[0]['pixel_values']))\n",
    "print(len(train_data))\n",
    "print(train_data.texts[0]['input_ids'].shape)\n",
    "\n",
    "model = Model()\n",
    "model.cuda()\n",
    "model.train()\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR_FULL)\n",
    "\n",
    "best_loss = 1e9\n",
    "\n",
    "for epoch in range(EPOCHS_FULL):\n",
    "\n",
    "    train_loss = 0.0    \n",
    "    model.train()\n",
    "    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        labels_batch = labels_batch.to(torch.float32)\n",
    "        labels_batch = labels_batch.to('cuda')\n",
    "\n",
    "        labels_predictions = model(images_batch, texts_batch)\n",
    "\n",
    "#         print(labels_predictions.shape)\n",
    "#         print(labels_batch.shape)\n",
    "        \n",
    "#         print(labels_predictions.type())\n",
    "#         print(labels_batch.type())\n",
    "        \n",
    "#         print(labels_predictions)\n",
    "#         print(labels_batch)\n",
    "        \n",
    "        loss = criterion(labels_predictions, labels_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = train_loss + loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    validation_loss = 0.0\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(valid_dataloader)):\n",
    "        labels_batch = labels_batch.to(torch.float32)\n",
    "        labels_batch = labels_batch.to('cuda')\n",
    "        labels_predictions = model(images_batch, texts_batch)\n",
    "\n",
    "        loss = criterion(labels_predictions, labels_batch)\n",
    "\n",
    "\n",
    "        validation_loss = validation_loss + loss.item()\n",
    "\n",
    "\n",
    "        predicted = (labels_predictions > 0.5)\n",
    "        \n",
    "        total += labels_batch.size(0)\n",
    "        correct += (predicted == labels_batch).sum().item()\n",
    "\n",
    "\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "    validation_loss /= len(train_dataloader.dataset)\n",
    "    accuracy = (correct / total) / len(bin_classes)\n",
    "    print(f'Epoch: {epoch} Train Loss: {train_loss} Validation Loss: {validation_loss} Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    # Save checkpoint if needed\n",
    "    # checkpoint = {'checkpoint': model.state_dict()}\n",
    "    # torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'checkpoint_{epoch}.pt'))\n",
    "    print(f'Checkpoint reached! Validation loss modified from {best_loss} to {validation_loss}')\n",
    "    best_loss = validation_loss\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "for param in model.text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.image_encoder.parameters():\n",
    "    param.requires_grad = False \n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR_FC)\n",
    "best_loss = 1e9\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS_FC):\n",
    "\n",
    "    train_loss = 0.0    \n",
    "    model.train()\n",
    "    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        labels_batch = labels_batch.to(torch.float32)\n",
    "        labels_batch = labels_batch.to('cuda')\n",
    "\n",
    "        labels_predictions = model(images_batch, texts_batch)\n",
    "\n",
    "        loss = criterion(labels_predictions, labels_batch)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss = train_loss + loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    validation_loss = 0.0\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(valid_dataloader)):\n",
    "        labels_batch = labels_batch.to(torch.float32)\n",
    "        labels_batch = labels_batch.to('cuda')\n",
    "\n",
    "        labels_predictions = model(images_batch, texts_batch)\n",
    "\n",
    "        loss = criterion(labels_predictions, labels_batch)\n",
    "\n",
    "\n",
    "        validation_loss = validation_loss + loss.item()\n",
    "\n",
    "\n",
    "        predicted = (labels_predictions > 0.5)\n",
    "        total += labels_batch.size(0)\n",
    "        correct += (predicted == labels_batch).sum().item()\n",
    "\n",
    "\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "    validation_loss /= len(train_dataloader.dataset)\n",
    "    accuracy = (correct / total) / len(bin_classes)\n",
    "    print(f'Epoch: {epoch} Train Loss: {train_loss} Validation Loss: {validation_loss} Validation Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save checkpoint if needed\n",
    "# checkpoint = {'checkpoint': model.state_dict()}\n",
    "# torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'fc_checkpoint_{epoch}.pt'))\n",
    "print(f'Checkpoint reached! Validation loss modified from {best_loss} to {validation_loss}')\n",
    "best_loss = validation_loss\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "checkpoint = {'checkpoint': model.state_dict()}\n",
    "torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'checkpoint_ROBERTA.pt'))\n",
    "\n",
    "#import torch\n",
    "# model.train()\n",
    "# checkpoint = torch.load(os.path.join(PATH_SAVE_MODEL, f'fc_checkpoint_{4}.pt'))\n",
    "\n",
    "# # Apply the state dictionary to the model\n",
    "# model.load_state_dict(checkpoint['checkpoint'])\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "ids = []\n",
    "\n",
    "for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(test_dataloader)):\n",
    "    model.eval()\n",
    "\n",
    "    labels_predictions = model(images_batch, texts_batch)\n",
    "\n",
    "\n",
    "    predicted = (labels_predictions > 0.25)[0]\n",
    "    \n",
    "    curr_id = test_data.ids[useless_id]\n",
    "    if curr_id not in predictions:\n",
    "        predictions[curr_id] = []\n",
    "        \n",
    "    idx_bin_class = 0\n",
    "    for bin_class in bin_classes:\n",
    "        if predicted[idx_bin_class]:\n",
    "            predictions[curr_id].append(bin_class)\n",
    "        idx_bin_class += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_json = []\n",
    "for k,v in predictions.items():\n",
    "    output_json.append({\"id\" : k, \"labels\" : v})\n",
    "\n",
    "with open(os.path.join(PATH_SAVE_SUBMISSION, \"submission_EN_EXTRACONTEST.txt\"),\"w\") as fout:\n",
    "    json.dump(output_json, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4291150,
     "sourceId": 7383307,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
