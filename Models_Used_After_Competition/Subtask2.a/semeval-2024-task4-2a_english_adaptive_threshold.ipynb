{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:38:14.433840Z","iopub.status.busy":"2024-01-11T22:38:14.433475Z","iopub.status.idle":"2024-01-11T22:38:14.441247Z","shell.execute_reply":"2024-01-11T22:38:14.440260Z","shell.execute_reply.started":"2024-01-11T22:38:14.433811Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\alex1\\anaconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","import PIL\n","import torchvision\n","import numpy\n","import pandas\n","import torch \n","import torch.optim as optim\n","import gc\n","from torch.optim.lr_scheduler import StepLR\n","import cv2\n","import os\n","import json\n","import numpy as np\n","from transformers import BertModel, BertTokenizer\n","import torch\n","import matplotlib.pyplot as plt\n","from transformers import AutoTokenizer, AutoModel\n","from transformers import T5EncoderModel\n","from transformers import GPT2Tokenizer, GPT2Model\n","from transformers import ViTImageProcessor, ViTModel\n","from PIL import Image\n","import requests\n","from tqdm import tqdm\n","import re \n","import string "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:38:14.455417Z","iopub.status.busy":"2024-01-11T22:38:14.454814Z","iopub.status.idle":"2024-01-11T22:38:14.462365Z","shell.execute_reply":"2024-01-11T22:38:14.461509Z","shell.execute_reply.started":"2024-01-11T22:38:14.455391Z"},"trusted":true},"outputs":[],"source":["PATH_DATASETS = \"../datasets\"\n","PATH_JSON_TRAIN = os.path.join(PATH_DATASETS, \"data/subtask2a/train.json\") \n","PATH_JSON_VAL = os.path.join(PATH_DATASETS, \"data/subtask2a/validation.json\") \n","PATH_JSON_DEV = os.path.join(PATH_DATASETS, \"dev_gold_labels/dev_subtask2a_en.json\") \n","PATH_JSON_TEST = os.path.join(PATH_DATASETS, \"test_data/english/en_subtask2a_test_unlabeled.json\") \n","\n","\n","PATH_IMG_TRAIN = os.path.join(PATH_DATASETS, \"train_images\") \n","PATH_IMG_VAL = os.path.join(PATH_DATASETS, \"validation_images\") \n","PATH_IMG_DEV = os.path.join(PATH_DATASETS, \"dev_images\") \n","PATH_IMG_TEST = os.path.join(PATH_DATASETS, \"test_images/subtask1_2a/english\") \n","\n","PATH_SAVE_MODEL = \"subtask2a_models\"\n","PATH_SAVE_SUBMISSION = \"subtask2a_submissions\"\n","\n","os.makedirs(PATH_SAVE_MODEL, exist_ok=True)\n","os.makedirs(PATH_SAVE_SUBMISSION, exist_ok=True)\n","\n","BERT_MODEL = 'limjiayi/bert-hateful-memes-expanded' \n","NUM_CLASSES = 22\n","\n","BATCH_SIZE = 8\n","\n","EPOCHS_FULL = 3\n","LR_FULL = 1e-5\n","\n","EPOCHS_FC = 0\n","LR_FC = 3e-6\n","\n","TRAIN_ALL = True"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:38:14.478247Z","iopub.status.busy":"2024-01-11T22:38:14.477990Z","iopub.status.idle":"2024-01-11T22:38:14.532899Z","shell.execute_reply":"2024-01-11T22:38:14.532040Z","shell.execute_reply.started":"2024-01-11T22:38:14.478225Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'id': '63292', 'text': \"This is why we're free\\\\n\\\\nThis is why we're safe\\\\n\", 'image': 'prop_meme_556.png', 'labels': ['Causal Oversimplification', 'Transfer', 'Flag-waving'], 'link': 'https://www.facebook.com/SilentmajorityDJT/photos/2119966118152814/'}\n"]}],"source":["data = json.load(open(PATH_JSON_TRAIN,\"r\",encoding='utf-8'))\n","\n","print(data[0])"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:38:14.535402Z","iopub.status.busy":"2024-01-11T22:38:14.534606Z","iopub.status.idle":"2024-01-11T22:38:14.539412Z","shell.execute_reply":"2024-01-11T22:38:14.538496Z","shell.execute_reply.started":"2024-01-11T22:38:14.535367Z"},"trusted":true},"outputs":[],"source":["def preprocess(text):\n","    return text"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:38:14.540859Z","iopub.status.busy":"2024-01-11T22:38:14.540569Z","iopub.status.idle":"2024-01-11T22:38:14.548395Z","shell.execute_reply":"2024-01-11T22:38:14.547619Z","shell.execute_reply.started":"2024-01-11T22:38:14.540836Z"},"trusted":true},"outputs":[],"source":["transform = torchvision.transforms.Compose([\n","                #torchvision.transforms.ToPILImage(),\n","                #torchvision.transforms.Resize((224,224),interpolation = PIL.Image.BICUBIC),\n","                #torchvision.transforms.ToTensor(),\n","                #torchvision.transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","            ])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:38:14.550131Z","iopub.status.busy":"2024-01-11T22:38:14.549836Z","iopub.status.idle":"2024-01-11T22:38:15.596290Z","shell.execute_reply":"2024-01-11T22:38:15.595378Z","shell.execute_reply.started":"2024-01-11T22:38:14.550107Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\alex1\\anaconda3\\envs\\ml\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n","  return self.fget.__get__(instance, owner)()\n","Some weights of BertModel were not initialized from the model checkpoint at limjiayi/bert-hateful-memes-expanded and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k',do_resize = True,do_rescale = True,do_normalize = True,image_mean = [0.5,0.5,0.5],image_std = [0.5,0.5,0.5])\n","\n","tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n","text_model = AutoModel.from_pretrained(BERT_MODEL)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:38:15.598685Z","iopub.status.busy":"2024-01-11T22:38:15.598407Z","iopub.status.idle":"2024-01-11T22:38:15.611407Z","shell.execute_reply":"2024-01-11T22:38:15.610650Z","shell.execute_reply.started":"2024-01-11T22:38:15.598660Z"},"trusted":true},"outputs":[],"source":["class MyDataset(Dataset):\n","    \n","    def __init__(self, paths_json_img, bin_classes):\n","        self.filenames = []\n","        self.texts = []\n","        self.images = []\n","        self.ids = []\n","        self.labels = []\n","        \n","        for path_json, path_img in paths_json_img:\n","            print(path_json)\n","            data = json.load(open(path_json,\"r\",encoding='utf-8'))\n","\n","            for x in tqdm(data):\n","                currentPath = os.path.join(path_img,x['image'])\n","\n","                self.ids.append(x['id'])\n","\n","                if 'labels' in x:\n","                    curr_labels = []\n","                    for bin_class in bin_classes:\n","                        if bin_class in x['labels']:\n","                            curr_labels.append(1)\n","                        else:\n","                            curr_labels.append(0)\n","                    self.labels.append(curr_labels)\n","                else:\n","                    self.labels.append([])\n","\n","                text = preprocess(x['text'])\n","                if text is None:\n","                    text = \"\"\n","                self.texts.append(tokenizer(text,return_tensors='pt',padding='max_length',max_length=128,truncation=True))\n","                self.filenames.append(x['image'])\n","\n","                currentImage = cv2.imread(currentPath)\n","                currentImage = torch.tensor(transform(currentImage)).unsqueeze(0)\n","                features = processor(currentImage)\n","                self.images.append(features)\n","\n","    def __len__(self):\n","        return len(self.images)\n","    \n","    def __getitem__(self,idx):\n","        text_tensors = {}\n","        for key, value in self.texts[idx].items():\n","            text_tensors[key] = value.cuda() if isinstance(value, torch.Tensor) else value\n","        \n","        \n","        image_tensors = {}\n","        for key, value in self.images[idx].items():\n","            image_tensors[key] = value.cuda() if isinstance(value, torch.Tensor) else value\n","        \n","        return ((image_tensors,text_tensors),torch.tensor(self.labels[idx]))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:38:15.612686Z","iopub.status.busy":"2024-01-11T22:38:15.612443Z","iopub.status.idle":"2024-01-11T22:38:15.635657Z","shell.execute_reply":"2024-01-11T22:38:15.634902Z","shell.execute_reply.started":"2024-01-11T22:38:15.612664Z"},"trusted":true},"outputs":[],"source":["#torchvision.models.efficientnet_b0(pretrained=True)\n","class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        # Define text and image encoders\n","        self.text_encoder = AutoModel.from_pretrained(BERT_MODEL)\n","        \n","        self.image_encoder = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","        \n","        self.fc = nn.Linear(249600, NUM_CLASSES)  # Adjust num_classes accordingly\n","    def forward(self,  images,text_input):\n","        # Process text input\n","        \n","        text_outputs = []\n","\n","        for i in range(text_input['input_ids'].shape[0]):\n","            x = dict()\n","            x['input_ids'] = text_input['input_ids'][i]\n","            x['token_type_ids'] = text_input['token_type_ids'][i]\n","            x['attention_mask'] = text_input['attention_mask'][i]\n","            text_outputs.append(self.text_encoder(**x).last_hidden_state)\n","            \n","            \n","        text_outputs = torch.stack(text_outputs)\n","        image_outputs = []\n","        \n","        for i in range(images['pixel_values'][0].shape[0]):\n","            x = dict()\n","            x['pixel_values'] = images['pixel_values'][0][i].unsqueeze(0).cuda()\n","          \n","            image_outputs.append(self.image_encoder(**x).last_hidden_state)\n","        \n","        image_outputs = torch.stack(image_outputs)\n","\n","        # Flatten and concatenate the outputs\n","        text_outputs = text_outputs.view(text_outputs.size(0), -1)\n","        \n","        image_outputs = image_outputs.view(image_outputs.size(0), -1)\n","        combined = torch.cat((text_outputs, image_outputs), dim=1)\n","        \n","        # Pass through fully connected layer\n","        output = nn.Sigmoid()(self.fc(nn.Tanh()(combined)))\n","        return output"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:38:15.637883Z","iopub.status.busy":"2024-01-11T22:38:15.637627Z","iopub.status.idle":"2024-01-11T22:38:15.689249Z","shell.execute_reply":"2024-01-11T22:38:15.688410Z","shell.execute_reply.started":"2024-01-11T22:38:15.637860Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["22 22\n","['Causal Oversimplification', 'Transfer', 'Flag-waving', 'Black-and-white Fallacy/Dictatorship', 'Smears', 'Loaded Language', 'Glittering generalities (Virtue)', 'Thought-terminating cliché', 'Whataboutism', 'Slogans', 'Doubt', 'Name calling/Labeling', 'Repetition', 'Appeal to authority', 'Appeal to (Strong) Emotions', 'Reductio ad hitlerum', 'Appeal to fear/prejudice', 'Exaggeration/Minimisation', \"Misrepresentation of Someone's Position (Straw Man)\", 'Obfuscation, Intentional vagueness, Confusion', 'Bandwagon', 'Presenting Irrelevant Data (Red Herring)']\n"]}],"source":["data = json.load(open(PATH_JSON_TRAIN,\"r\",encoding='utf-8'))\n","\n","bin_classes = []\n","\n","for x in data:\n","    for label in x['labels']:\n","        if label not in bin_classes:\n","            bin_classes.append(label)\n","\n","print(len(bin_classes), NUM_CLASSES)\n","print(bin_classes)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:38:15.690761Z","iopub.status.busy":"2024-01-11T22:38:15.690409Z","iopub.status.idle":"2024-01-11T22:43:10.720823Z","shell.execute_reply":"2024-01-11T22:43:10.720062Z","shell.execute_reply.started":"2024-01-11T22:38:15.690728Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["../datasets\\data/subtask2a/train.json\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/7000 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 7000/7000 [01:46<00:00, 65.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["../datasets\\data/subtask2a/validation.json\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [00:07<00:00, 63.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["../datasets\\dev_gold_labels/dev_subtask2a_en.json\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1000/1000 [00:15<00:00, 64.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["../datasets\\data/subtask2a/validation.json\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [00:07<00:00, 63.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["../datasets\\test_data/english/en_subtask2a_test_unlabeled.json\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1500/1500 [00:22<00:00, 65.24it/s]\n"]}],"source":["if TRAIN_ALL:\n","    train_data = MyDataset([(PATH_JSON_TRAIN, PATH_IMG_TRAIN), (PATH_JSON_VAL, PATH_IMG_VAL), (PATH_JSON_DEV, PATH_IMG_DEV)], bin_classes)\n","    # train_data = MyDataset([(PATH_JSON_VAL, PATH_IMG_VAL)], bin_classes)\n","else:\n","    # train_data = MyDataset([(PATH_JSON_DEV, PATH_IMG_DEV)], bin_classes)\n","    train_data = MyDataset([(PATH_JSON_TRAIN, PATH_IMG_TRAIN), (PATH_JSON_DEV, PATH_IMG_DEV)], bin_classes)\n","valid_data = MyDataset([(PATH_JSON_VAL, PATH_IMG_VAL)], bin_classes)\n","test_data = MyDataset([(PATH_JSON_TEST, PATH_IMG_TEST)], bin_classes)\n","\n","train_dataloader = DataLoader(dataset = train_data, batch_size = BATCH_SIZE, shuffle = True)\n","valid_dataloader = DataLoader(dataset = valid_data, batch_size = BATCH_SIZE, shuffle = False)\n","test_dataloader = DataLoader(dataset = test_data, batch_size = 1, shuffle = False)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:43:10.723055Z","iopub.status.busy":"2024-01-11T22:43:10.722806Z","iopub.status.idle":"2024-01-11T22:43:10.727160Z","shell.execute_reply":"2024-01-11T22:43:10.726242Z","shell.execute_reply.started":"2024-01-11T22:43:10.723033Z"},"trusted":true},"outputs":[],"source":["# m = nn.Sigmoid()\n","# loss = nn.BCELoss()\n","# input = torch.randn(3, requires_grad=True)\n","# target = torch.empty(3).random_(2)\n","\n","# print(input)\n","# print(target)\n","# output = loss(m(input), target)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T22:43:10.728861Z","iopub.status.busy":"2024-01-11T22:43:10.728426Z","iopub.status.idle":"2024-01-11T23:07:05.155687Z","shell.execute_reply":"2024-01-11T23:07:05.154609Z","shell.execute_reply.started":"2024-01-11T22:43:10.728836Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","8500\n","torch.Size([1, 128])\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertModel were not initialized from the model checkpoint at limjiayi/bert-hateful-memes-expanded and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","1063it [07:25,  2.39it/s]\n","63it [00:07,  8.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["BEST THRESHOLDS\n","Causal Oversimplification : best_thresh=0.13298802077770233 , best_f1=0.43636363636363634\n","Transfer : best_thresh=0.3453764319419861 , best_f1=0.6885245901639344\n","Flag-waving : best_thresh=0.2096594274044037 , best_f1=0.7394957983193278\n","Black-and-white Fallacy/Dictatorship : best_thresh=0.5069349408149719 , best_f1=0.6382978723404256\n","Smears : best_thresh=0.5679747462272644 , best_f1=0.8200371057513914\n","Loaded Language : best_thresh=0.3873363435268402 , best_f1=0.7035830618892508\n","Glittering generalities (Virtue) : best_thresh=0.11461390554904938 , best_f1=0.5693430656934306\n","Thought-terminating cliché : best_thresh=0.31492480635643005 , best_f1=0.5396825396825397\n","Whataboutism : best_thresh=0.09735649079084396 , best_f1=0.5555555555555556\n","Slogans : best_thresh=0.13925980031490326 , best_f1=0.5714285714285714\n","Doubt : best_thresh=0.2307792752981186 , best_f1=0.6037735849056604\n","Name calling/Labeling : best_thresh=0.3842535614967346 , best_f1=0.6842105263157895\n","Repetition : best_thresh=0.4247414171695709 , best_f1=0.6666666666666666\n","Appeal to authority : best_thresh=0.4437524676322937 , best_f1=0.8923076923076924\n","Appeal to (Strong) Emotions : best_thresh=0.45307692885398865 , best_f1=0.7307692307692307\n","Reductio ad hitlerum : best_thresh=0.11320895701646805 , best_f1=0.43478260869565216\n","Appeal to fear/prejudice : best_thresh=0.31644290685653687 , best_f1=0.6101694915254238\n","Exaggeration/Minimisation : best_thresh=0.3362809717655182 , best_f1=0.5\n","Misrepresentation of Someone's Position (Straw Man) : best_thresh=0.06012238562107086 , best_f1=0.7272727272727273\n","Obfuscation, Intentional vagueness, Confusion : best_thresh=0.04856697842478752 , best_f1=0.3333333333333333\n","Bandwagon : best_thresh=0.4257144629955292 , best_f1=0.5454545454545454\n","Presenting Irrelevant Data (Red Herring) : best_thresh=0.08484117686748505 , best_f1=0.5714285714285714\n","\n","Epoch: 0 Train Loss: 0.029356491620049757 Validation Loss: 0.0013313606267466266 Validation Accuracy thresh=0.5: 93.17%\n"]},{"name":"stderr","output_type":"stream","text":["1063it [07:17,  2.43it/s]\n","63it [00:07,  8.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["BEST THRESHOLDS\n","Causal Oversimplification : best_thresh=0.16787345707416534 , best_f1=0.8\n","Transfer : best_thresh=0.3084378242492676 , best_f1=0.865546218487395\n","Flag-waving : best_thresh=0.3643975555896759 , best_f1=0.8623853211009175\n","Black-and-white Fallacy/Dictatorship : best_thresh=0.37753722071647644 , best_f1=0.8785046728971962\n","Smears : best_thresh=0.4502951502799988 , best_f1=0.90625\n","Loaded Language : best_thresh=0.6506150364875793 , best_f1=0.8148148148148148\n","Glittering generalities (Virtue) : best_thresh=0.45759671926498413 , best_f1=0.8163265306122449\n","Thought-terminating cliché : best_thresh=0.22561617195606232 , best_f1=0.7945205479452054\n","Whataboutism : best_thresh=0.2015017420053482 , best_f1=0.88\n","Slogans : best_thresh=0.26299503445625305 , best_f1=0.8076923076923077\n","Doubt : best_thresh=0.23064018785953522 , best_f1=0.7407407407407407\n","Name calling/Labeling : best_thresh=0.3651345372200012 , best_f1=0.9024390243902439\n","Repetition : best_thresh=0.4935813248157501 , best_f1=0.8636363636363636\n","Appeal to authority : best_thresh=0.3522473871707916 , best_f1=0.9558823529411765\n","Appeal to (Strong) Emotions : best_thresh=0.3821240961551666 , best_f1=0.8813559322033898\n","Reductio ad hitlerum : best_thresh=0.16362251341342926 , best_f1=0.8421052631578947\n","Appeal to fear/prejudice : best_thresh=0.2746584713459015 , best_f1=0.8253968253968254\n","Exaggeration/Minimisation : best_thresh=0.21821606159210205 , best_f1=0.8148148148148148\n","Misrepresentation of Someone's Position (Straw Man) : best_thresh=0.05593309924006462 , best_f1=1.0\n","Obfuscation, Intentional vagueness, Confusion : best_thresh=0.06241395324468613 , best_f1=0.9090909090909091\n","Bandwagon : best_thresh=0.2857167422771454 , best_f1=0.9333333333333333\n","Presenting Irrelevant Data (Red Herring) : best_thresh=0.10613191872835159 , best_f1=0.8\n","\n","Epoch: 1 Train Loss: 0.021247333236476953 Validation Loss: 0.0008502151659306357 Validation Accuracy thresh=0.5: 95.79%\n"]},{"name":"stderr","output_type":"stream","text":["1063it [07:15,  2.44it/s]\n","63it [00:07,  8.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["BEST THRESHOLDS\n","Causal Oversimplification : best_thresh=0.41665977239608765 , best_f1=0.9047619047619048\n","Transfer : best_thresh=0.4611648619174957 , best_f1=0.9626556016597511\n","Flag-waving : best_thresh=0.4242860972881317 , best_f1=0.957983193277311\n","Black-and-white Fallacy/Dictatorship : best_thresh=0.4716000258922577 , best_f1=0.9464285714285714\n","Smears : best_thresh=0.3237995207309723 , best_f1=0.9659090909090909\n","Loaded Language : best_thresh=0.5875219702720642 , best_f1=0.9191176470588235\n","Glittering generalities (Virtue) : best_thresh=0.35387077927589417 , best_f1=0.9696969696969697\n","Thought-terminating cliché : best_thresh=0.4726349115371704 , best_f1=0.9459459459459459\n","Whataboutism : best_thresh=0.28403353691101074 , best_f1=0.9795918367346939\n","Slogans : best_thresh=0.30799734592437744 , best_f1=0.9357798165137615\n","Doubt : best_thresh=0.4400121569633484 , best_f1=0.9333333333333333\n","Name calling/Labeling : best_thresh=0.19074812531471252 , best_f1=0.9590163934426229\n","Repetition : best_thresh=0.25371190905570984 , best_f1=0.9777777777777777\n","Appeal to authority : best_thresh=0.21047121286392212 , best_f1=0.9924812030075187\n","Appeal to (Strong) Emotions : best_thresh=0.5627464652061462 , best_f1=1.0\n","Reductio ad hitlerum : best_thresh=0.13264508545398712 , best_f1=1.0\n","Appeal to fear/prejudice : best_thresh=0.43526163697242737 , best_f1=0.9393939393939394\n","Exaggeration/Minimisation : best_thresh=0.331757515668869 , best_f1=0.9830508474576272\n","Misrepresentation of Someone's Position (Straw Man) : best_thresh=0.4981842041015625 , best_f1=1.0\n","Obfuscation, Intentional vagueness, Confusion : best_thresh=0.07887764275074005 , best_f1=1.0\n","Bandwagon : best_thresh=0.15109647810459137 , best_f1=1.0\n","Presenting Irrelevant Data (Red Herring) : best_thresh=0.13876338303089142 , best_f1=1.0\n","\n","Epoch: 2 Train Loss: 0.014127974746858372 Validation Loss: 0.00044815769725862673 Validation Accuracy thresh=0.5: 98.72%\n","Checkpoint reached! Validation loss modified from 1000000000.0 to 0.00044815769725862673\n","Checkpoint reached! Validation loss modified from 1000000000.0 to 0.00044815769725862673\n"]}],"source":["print(len(train_data.images[0]['pixel_values']))\n","print(len(train_data))\n","print(train_data.texts[0]['input_ids'].shape)\n","\n","model = Model()\n","model.cuda()\n","model.train()\n","\n","best_thresh_all = []\n","\n","criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = LR_FULL)\n","\n","best_loss = 1e9\n","\n","for epoch in range(EPOCHS_FULL):\n","\n","    train_loss = 0.0    \n","    model.train()\n","    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(train_dataloader)):\n","        optimizer.zero_grad()\n","\n","        labels_batch = labels_batch.to(torch.float32)\n","        labels_batch = labels_batch.to('cuda')\n","\n","        labels_predictions = model(images_batch, texts_batch)\n","\n","#         print(labels_predictions.shape)\n","#         print(labels_batch.shape)\n","        \n","#         print(labels_predictions.type())\n","#         print(labels_batch.type())\n","        \n","#         print(labels_predictions)\n","#         print(labels_batch)\n","        \n","        loss = criterion(labels_predictions, labels_batch)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        train_loss = train_loss + loss.item()\n","\n","    # Validation loop\n","    validation_loss = 0.0\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    all_val_pred = [[] for _ in range(NUM_CLASSES)] \n","    all_val_gt = [[] for _ in range(NUM_CLASSES)] \n","\n","    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(valid_dataloader)):\n","        labels_batch = labels_batch.to(torch.float32)\n","        labels_batch = labels_batch.to('cuda')\n","        labels_predictions = model(images_batch, texts_batch)\n","\n","        loss = criterion(labels_predictions, labels_batch)\n","\n","\n","        validation_loss = validation_loss + loss.item()\n","\n","        predicted = (labels_predictions > 0.5)\n","        \n","        total += labels_batch.size(0)\n","        correct += (predicted == labels_batch).sum().item()\n","\n","        cpu_labels_predictions = labels_predictions.to('cpu').tolist()\n","        cpu_labels_batch = labels_batch.to('cpu').tolist()\n","\n","        for bat in range(len(cpu_labels_predictions)):\n","            for i in range(NUM_CLASSES):\n","                all_val_pred[i].append(cpu_labels_predictions[bat][i])\n","                all_val_gt[i].append(cpu_labels_batch[bat][i])\n","\n","\n","    best_thresh_all = []\n","    print(\"BEST THRESHOLDS\")\n","    for i in range(NUM_CLASSES):\n","        zipped_pred_gt = list(zip(all_val_pred[i], all_val_gt[i]))\n","        zipped_pred_gt.sort()\n","\n","        best_thresh = 0\n","        best_f1 = 0\n","        tp = sum(all_val_gt[i])\n","        fp = len(all_val_gt[i]) - tp\n","        fn = 0\n","        for x in zipped_pred_gt:\n","            if x[1] == 1:\n","                tp -= 1\n","                fn += 1\n","            else:\n","                fp -= 1\n","\n","            if tp > 0:\n","                curr_f1 = 2*tp / (2*tp + fp + fn)\n","                if curr_f1 > best_f1:\n","                    best_f1 = curr_f1\n","                    best_thresh = x[0] \n","        best_thresh_all.append(best_thresh)\n","\n","        print(f\"{bin_classes[i]} : best_thresh={best_thresh} , best_f1={best_f1}\")\n","    print()\n","\n","    train_loss /= len(train_dataloader.dataset)\n","    validation_loss /= len(train_dataloader.dataset)\n","    accuracy = (correct / total) / len(bin_classes)\n","    print(f'Epoch: {epoch} Train Loss: {train_loss} Validation Loss: {validation_loss} Validation Accuracy thresh=0.5: {accuracy * 100:.2f}%')\n","\n","# Save checkpoint if needed\n","# checkpoint = {'checkpoint': model.state_dict()}\n","# torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'checkpoint_{epoch}.pt'))\n","print(f'Checkpoint reached! Validation loss modified from {best_loss} to {validation_loss}')\n","best_loss = validation_loss\n","torch.cuda.empty_cache()\n","\n","    \n","    \n","    \n","for param in model.text_encoder.parameters():\n","    param.requires_grad = False\n","\n","for param in model.image_encoder.parameters():\n","    param.requires_grad = False \n","\n","optimizer = torch.optim.Adam(model.parameters(), lr = LR_FC)\n","best_loss = 1e9\n","\n","\n","for epoch in range(EPOCHS_FC):\n","\n","    train_loss = 0.0    \n","    model.train()\n","    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(train_dataloader)):\n","        optimizer.zero_grad()\n","\n","        labels_batch = labels_batch.to(torch.float32)\n","        labels_batch = labels_batch.to('cuda')\n","\n","        labels_predictions = model(images_batch, texts_batch)\n","\n","        loss = criterion(labels_predictions, labels_batch)\n","        loss.backward()\n","\n","        optimizer.step()\n","\n","        train_loss = train_loss + loss.item()\n","\n","    # Validation loop\n","    validation_loss = 0.0\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(valid_dataloader)):\n","        labels_batch = labels_batch.to(torch.float32)\n","        labels_batch = labels_batch.to('cuda')\n","\n","        labels_predictions = model(images_batch, texts_batch)\n","        loss = criterion(labels_predictions, labels_batch)\n","        validation_loss = validation_loss + loss.item()\n","\n","        predicted = (labels_predictions > 0.5)\n","        total += labels_batch.size(0)\n","        correct += (predicted == labels_batch).sum().item()\n","\n","\n","    train_loss /= len(train_dataloader.dataset)\n","    validation_loss /= len(train_dataloader.dataset)\n","    accuracy = (correct / total) / len(bin_classes)\n","\n","    print(f'Epoch: {epoch} Train Loss: {train_loss} Validation Loss: {validation_loss} Validation Accuracy thresh=0.5: {accuracy * 100:.2f}%')\n","\n","\n","\n","\n","\n","# Save checkpoint if needed\n","# checkpoint = {'checkpoint': model.state_dict()}\n","# torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'fc_checkpoint_{epoch}.pt'))\n","print(f'Checkpoint reached! Validation loss modified from {best_loss} to {validation_loss}')\n","best_loss = validation_loss\n","torch.cuda.empty_cache()\n","\n","checkpoint = {'checkpoint': model.state_dict()}\n","torch.save(checkpoint, os.path.join(PATH_SAVE_MODEL, f'checkpoint.pt'))\n","\n","#import torch\n","# model.train()\n","# checkpoint = torch.load(os.path.join(PATH_SAVE_MODEL, f'fc_checkpoint_{4}.pt'))\n","\n","# # Apply the state dictionary to the model\n","# model.load_state_dict(checkpoint['checkpoint'])\n","                    \n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0.25, 0.25, 0.4325084984302521, 0.25856152176856995, 0.25, 0.35122835636138916, 0.25, 0.36188608407974243, 0.25, 0.25, 0.36091744899749756, 0.28508374094963074, 0.2983148694038391, 0.462566077709198, 0.25, 0.5132802724838257, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n"]}],"source":["print(best_thresh_all)\n","\n","best_thresh_all = [0.07052270323038101, 0.2133808732032776, 0.4325084984302521, 0.25856152176856995, 0.07286142557859421, 0.35122835636138916, 0.20894578099250793, 0.36188608407974243, 0.17205330729484558, 0.2361125946044922, 0.36091744899749756, 0.28508374094963074, 0.2983148694038391, 0.462566077709198, 0.07985731214284897, 0.5132802724838257, 0.07505106925964355, 0.18899203836917877, 0.12206489592790604, 0.033173780888319016, 0.10967455059289932, 0.020485831424593925]\n","\n","best_thresh_all = [max(0.2, x) * 1 for x in best_thresh_all]"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T23:30:20.179266Z","iopub.status.busy":"2024-01-11T23:30:20.178234Z","iopub.status.idle":"2024-01-11T23:30:45.325929Z","shell.execute_reply":"2024-01-11T23:30:45.325049Z","shell.execute_reply.started":"2024-01-11T23:30:20.179224Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["1500it [00:24, 60.78it/s]\n"]}],"source":["predictions = {}\n","ids = []\n","\n","for useless_id, ((images_batch, texts_batch), labels_batch) in tqdm(enumerate(test_dataloader)):\n","    model.eval()\n","\n","    labels_predictions = model(images_batch, texts_batch)\n","\n","\n","    predicted = labels_predictions[0]\n","    \n","    curr_id = test_data.ids[useless_id]\n","    if curr_id not in predictions:\n","        predictions[curr_id] = []\n","        \n","    idx_bin_class = 0\n","    for bin_class in bin_classes:\n","        if predicted[idx_bin_class] > best_thresh_all[idx_bin_class]:\n","            predictions[curr_id].append(bin_class)\n","        idx_bin_class += 1"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-01-11T23:30:45.328298Z","iopub.status.busy":"2024-01-11T23:30:45.327925Z","iopub.status.idle":"2024-01-11T23:30:45.345841Z","shell.execute_reply":"2024-01-11T23:30:45.344929Z","shell.execute_reply.started":"2024-01-11T23:30:45.328264Z"},"trusted":true},"outputs":[],"source":["output_json = []\n","for k,v in predictions.items():\n","    output_json.append({\"id\" : k, \"labels\" : v})\n","\n","with open(os.path.join(PATH_SAVE_SUBMISSION, \"submission.txt\"),\"w\") as fout:\n","    json.dump(output_json, fout)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4291150,"sourceId":7383307,"sourceType":"datasetVersion"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
